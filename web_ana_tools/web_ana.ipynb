{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b35695f",
   "metadata": {},
   "source": [
    "#### 读取网页HTML ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dc441e",
   "metadata": {},
   "source": [
    "##### V0.2: leaf_nodes, interactive_elements #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c7ced7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/allen1997/miniconda3/envs/myenv/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n",
      "/tmp/ipykernel_3043/3425569868.py:57: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(html_content, 'html.parser')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "统计完成，数据已保存为 web_layout_interaction_panel.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "\n",
    "# 定义布局标签\n",
    "target_tags = ['div', 'section', 'article', 'aside', 'nav']\n",
    "\n",
    "# 统计交互元素\n",
    "def count_interactive_elements(soup):\n",
    "    a_tags = len(soup.find_all('a', href=True))\n",
    "    button_tags = len(soup.find_all('button'))\n",
    "    onclick_tags = len([tag for tag in soup.find_all(attrs={'onclick': True}) if tag.name not in ['a', 'button']])\n",
    "    input_tags = len(soup.find_all(['input', 'select', 'textarea']))\n",
    "    return a_tags + button_tags + onclick_tags + input_tags\n",
    "\n",
    "# 遍历DOM，统计层级与叶子节点\n",
    "def traverse_dom(node, current_level, level_counter, leaf_counter, max_level):\n",
    "    if node.name in target_tags:\n",
    "        level = min(current_level, 20)  # 超过L20归为L20\n",
    "        level_counter[level] += 1\n",
    "        if not any(child.name in target_tags for child in node.find_all(recursive=False)):\n",
    "            leaf_counter[0] += 1\n",
    "        if current_level > max_level[0]:\n",
    "            max_level[0] = current_level\n",
    "    for child in node.find_all(recursive=False):\n",
    "        traverse_dom(child, current_level + 1, level_counter, leaf_counter, max_level)\n",
    "\n",
    "# 主程序\n",
    "results = []\n",
    "\n",
    "root_dir = '/home/allen1997/wayback_machine_downloader/websites/'\n",
    "\n",
    "for website in os.listdir(root_dir):\n",
    "    website_path = os.path.join(root_dir, website)\n",
    "    if not os.path.isdir(website_path):\n",
    "        continue\n",
    "    \n",
    "    for year in range(2009, 2020):\n",
    "        year_dir = os.path.join(website_path, str(year))\n",
    "        if not os.path.exists(year_dir):\n",
    "            results.append([website, year] + ['NA']*23)\n",
    "            continue\n",
    "        \n",
    "        # 匹配形如 *_index.html 的文件\n",
    "        matched_files = glob.glob(os.path.join(year_dir, '*_index.html'))\n",
    "        if matched_files:\n",
    "            year_path = matched_files[0]  # 如果有多个，只取第一个\n",
    "        else:\n",
    "            results.append([website, year] + ['NA']*23) # 没有则置空\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            with open(year_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                html_content = file.read()\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            \n",
    "            level_counter = defaultdict(int)\n",
    "            leaf_counter = [0]\n",
    "            max_level = [0]\n",
    "            \n",
    "            body = soup.body\n",
    "            if body:\n",
    "                traverse_dom(body, 0, level_counter, leaf_counter, max_level)\n",
    "            \n",
    "            levels = [level_counter[i] for i in range(1, 21)]\n",
    "            interactive = count_interactive_elements(soup)\n",
    "            \n",
    "            results.append([website, year, max_level[0]] + levels + [leaf_counter[0], interactive])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            results.append([website, year] + ['ERROR']*23)\n",
    "\n",
    "columns = ['website', 'year', 'max_depth'] + [f'L{i}' for i in range(1, 21)] + ['leaf_nodes', 'interactive_elements']\n",
    "df = pd.DataFrame(results, columns=columns)\n",
    "df.to_excel('outputs/web_layout_interaction_panel.xlsx', index=False)\n",
    "\n",
    "print(\"统计完成，数据已保存为 web_layout_interaction_panel.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6143a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon.com 2009 11 182 3 2 284 210\n",
      "Amazon.com 2010 11 293 11 7 508 251\n",
      "Amazon.com 2011 10 205 11 8 259 205\n",
      "Amazon.com 2012 11 223 4 5 399 376\n",
      "Amazon.com 2013 9 169 4 6 251 412\n",
      "Amazon.com 2014 12 96 2 1 207 448\n",
      "Amazon.com 2015 13 67 1 1 170 273\n",
      "Amazon.com 2016 20 56 1 3 154 121\n",
      "Amazon.com 2017 14 119 1 4 244 193\n",
      "Amazon.com 2018 0 0 0 0 0 0\n",
      "Amazon.com 2019 11 239 6 9 456 242\n",
      "统计完成，数据已保存为 web_layout_interaction_panel.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 定义模块关键词\n",
    "effective_keywords = ['module', 'block', 'wrapper', 'container', 'card', 'panel', 'section', 'content', 'box']\n",
    "\n",
    "# 定义交互元素标签\n",
    "def count_interactive_elements(soup):\n",
    "    a_tags = len(soup.find_all('a', href=True))\n",
    "    button_tags = len(soup.find_all('button'))\n",
    "    onclick_tags = len([tag for tag in soup.find_all(attrs={'onclick': True}) if tag.name not in ['a', 'button']])\n",
    "    input_tags = len(soup.find_all(['input', 'select', 'textarea']))\n",
    "    return a_tags + button_tags + onclick_tags + input_tags\n",
    "\n",
    "# 计算最大嵌套层级\n",
    "def calculate_max_depth(modules):\n",
    "    max_depth = 0\n",
    "    for module in modules:\n",
    "        depth = 1\n",
    "        parent = module.find_parent()\n",
    "        while parent:\n",
    "            if parent in modules:\n",
    "                depth += 1\n",
    "            parent = parent.find_parent()\n",
    "        if depth > max_depth:\n",
    "            max_depth = depth\n",
    "    return max_depth\n",
    "\n",
    "# 计算当前节点所处层级\n",
    "def get_module_level_in_list(element, modules):\n",
    "    level = 0\n",
    "    current = element\n",
    "    while current.parent is not None:\n",
    "        current = current.parent\n",
    "        if current in modules:\n",
    "            level += 1\n",
    "        else:\n",
    "            break\n",
    "    return level\n",
    "\n",
    "# 统计叶子模块\n",
    "def count_leaf_modules(modules):\n",
    "    leaf_modules = [m for m in modules if not any(child in modules for child in m.find_all(['div', 'section', 'article', 'aside', 'nav']))]\n",
    "    \n",
    "    # 统计层级\n",
    "    leaf_info = []\n",
    "    for leaf in leaf_modules:\n",
    "        level = get_module_level_in_list(leaf, modules)\n",
    "        leaf_info.append({'element': leaf, 'level': level})\n",
    "    \n",
    "        # print(f\"标签: {leaf.name}, 层级: {level}\")\n",
    "\n",
    "    return len(leaf_modules), leaf_info  \n",
    "\n",
    "# 判断是否为有效模块\n",
    "def is_effective_module(tag):\n",
    "    if not tag.has_attr('class') and not tag.has_attr('id'):\n",
    "        return False\n",
    "    identifiers = []\n",
    "    if tag.has_attr('class'):\n",
    "        identifiers.extend(tag.get('class'))\n",
    "    if tag.has_attr('id'):\n",
    "        identifiers.append(tag.get('id'))\n",
    "    # print(identifiers)\n",
    "    # return any(kw in ident for ident in identifiers for kw in effective_keywords)\n",
    "    return True\n",
    "\n",
    "# 统计模块层级\n",
    "def count_module_levels(modules):\n",
    "    level1 = [\n",
    "        m for m in modules \n",
    "        if not any(parent in modules for parent in m.find_parents(['div', 'section', 'article', 'aside', 'nav']))\n",
    "    ]\n",
    "    level2 = [m for m in modules if m.find_parent() in level1]\n",
    "    level3plus = len(modules) - len(level1) - len(level2)\n",
    "    \n",
    "    return len(level1), len(level2), level3plus\n",
    "\n",
    "# 主程序\n",
    "results = []\n",
    "\n",
    "root_dir = './test'\n",
    "\n",
    "for website in os.listdir(root_dir):\n",
    "    website_path = os.path.join(root_dir, website)\n",
    "    if not os.path.isdir(website_path):\n",
    "        continue\n",
    "    \n",
    "    for year in range(2009, 2020):\n",
    "        year_path = os.path.join(website_path, str(year), 'index.html')\n",
    "        if not os.path.exists(year_path):\n",
    "            results.append([website, year, 'NA', 'NA', 'NA', 'NA'])\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            with open(year_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                html_content = file.read()\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            \n",
    "\n",
    "            modules = [tag for tag in soup.find_all(['div', 'section', 'article', 'aside', 'nav']) if is_effective_module(tag)]\n",
    "\n",
    "            max_depth = calculate_max_depth(modules) # 最大嵌套层级\n",
    "            leaf_count, leaf_info = count_leaf_modules(modules) # 叶子布局节点数量\n",
    "            l1, l2, l3 = count_module_levels(modules) # l1-l3布局节点数量\n",
    "\n",
    "            interactive = count_interactive_elements(soup) # 交互节点数量\n",
    "\n",
    "            print(website, year, max_depth, leaf_count, l1, l2, l3, interactive)\n",
    "            \n",
    "            results.append([website, year, max_depth, leaf_count, l1, l2, l3, interactive])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            results.append([website, year, 'ERROR', 'ERROR', 'ERROR', 'ERROR', 'ERROR', 'ERROR'])\n",
    "\n",
    "# 保存为CSV\n",
    "df = pd.DataFrame(results, columns=['website', 'year', 'max_depth', 'leaf_count', 'level1_modules', 'level2_modules', 'level3plus_modules', 'interactive_elements'])\n",
    "df.to_excel('web_layout_interaction_panel.xlsx', index=False)\n",
    "\n",
    "print(\"统计完成，数据已保存为 web_layout_interaction_panel.xlsx\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
